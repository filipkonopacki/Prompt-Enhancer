{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.9771598808341606,
  "eval_steps": 500,
  "global_step": 1500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.019860973187686197,
      "grad_norm": 5.253796100616455,
      "learning_rate": 1.1920529801324503e-05,
      "loss": 36.7976,
      "step": 10
    },
    {
      "epoch": 0.039721946375372394,
      "grad_norm": 4.837586879730225,
      "learning_rate": 2.5165562913907287e-05,
      "loss": 36.5831,
      "step": 20
    },
    {
      "epoch": 0.05958291956305859,
      "grad_norm": 6.875422477722168,
      "learning_rate": 3.841059602649007e-05,
      "loss": 36.0786,
      "step": 30
    },
    {
      "epoch": 0.07944389275074479,
      "grad_norm": 8.088799476623535,
      "learning_rate": 5.165562913907285e-05,
      "loss": 35.4747,
      "step": 40
    },
    {
      "epoch": 0.09930486593843098,
      "grad_norm": 9.880268096923828,
      "learning_rate": 6.490066225165563e-05,
      "loss": 34.6956,
      "step": 50
    },
    {
      "epoch": 0.11916583912611718,
      "grad_norm": 10.008866310119629,
      "learning_rate": 7.814569536423842e-05,
      "loss": 32.7875,
      "step": 60
    },
    {
      "epoch": 0.13902681231380337,
      "grad_norm": 10.382579803466797,
      "learning_rate": 9.13907284768212e-05,
      "loss": 29.8254,
      "step": 70
    },
    {
      "epoch": 0.15888778550148958,
      "grad_norm": 6.888306140899658,
      "learning_rate": 0.00010463576158940399,
      "loss": 27.5659,
      "step": 80
    },
    {
      "epoch": 0.17874875868917578,
      "grad_norm": 5.732194423675537,
      "learning_rate": 0.00011788079470198677,
      "loss": 24.5932,
      "step": 90
    },
    {
      "epoch": 0.19860973187686196,
      "grad_norm": 5.367650032043457,
      "learning_rate": 0.00013112582781456955,
      "loss": 21.1126,
      "step": 100
    },
    {
      "epoch": 0.21847070506454816,
      "grad_norm": 6.390696048736572,
      "learning_rate": 0.00014437086092715232,
      "loss": 17.329,
      "step": 110
    },
    {
      "epoch": 0.23833167825223436,
      "grad_norm": 7.014334201812744,
      "learning_rate": 0.00015761589403973512,
      "loss": 12.3424,
      "step": 120
    },
    {
      "epoch": 0.25819265143992054,
      "grad_norm": 2.5403597354888916,
      "learning_rate": 0.0001708609271523179,
      "loss": 6.8942,
      "step": 130
    },
    {
      "epoch": 0.27805362462760674,
      "grad_norm": 0.7649480104446411,
      "learning_rate": 0.0001841059602649007,
      "loss": 4.9534,
      "step": 140
    },
    {
      "epoch": 0.29791459781529295,
      "grad_norm": 0.34956544637680054,
      "learning_rate": 0.00019735099337748346,
      "loss": 4.4504,
      "step": 150
    },
    {
      "epoch": 0.31777557100297915,
      "grad_norm": 0.2962968647480011,
      "learning_rate": 0.0001988217967599411,
      "loss": 4.2538,
      "step": 160
    },
    {
      "epoch": 0.33763654419066536,
      "grad_norm": 0.29888492822647095,
      "learning_rate": 0.00019734904270986747,
      "loss": 4.0925,
      "step": 170
    },
    {
      "epoch": 0.35749751737835156,
      "grad_norm": 0.4724421501159668,
      "learning_rate": 0.00019587628865979381,
      "loss": 3.9474,
      "step": 180
    },
    {
      "epoch": 0.37735849056603776,
      "grad_norm": 0.6244088411331177,
      "learning_rate": 0.00019440353460972018,
      "loss": 3.7316,
      "step": 190
    },
    {
      "epoch": 0.3972194637537239,
      "grad_norm": 0.8345080018043518,
      "learning_rate": 0.00019293078055964655,
      "loss": 3.3699,
      "step": 200
    },
    {
      "epoch": 0.4170804369414101,
      "grad_norm": 0.5824906229972839,
      "learning_rate": 0.00019145802650957292,
      "loss": 3.0011,
      "step": 210
    },
    {
      "epoch": 0.4369414101290963,
      "grad_norm": 0.5638323426246643,
      "learning_rate": 0.00018998527245949926,
      "loss": 2.663,
      "step": 220
    },
    {
      "epoch": 0.4568023833167825,
      "grad_norm": 0.5772266983985901,
      "learning_rate": 0.00018851251840942563,
      "loss": 2.3238,
      "step": 230
    },
    {
      "epoch": 0.4766633565044687,
      "grad_norm": 0.49838361144065857,
      "learning_rate": 0.000187039764359352,
      "loss": 2.0815,
      "step": 240
    },
    {
      "epoch": 0.49652432969215493,
      "grad_norm": 0.37251222133636475,
      "learning_rate": 0.00018556701030927837,
      "loss": 1.8469,
      "step": 250
    },
    {
      "epoch": 0.5163853028798411,
      "grad_norm": 0.3853132724761963,
      "learning_rate": 0.0001840942562592047,
      "loss": 1.7161,
      "step": 260
    },
    {
      "epoch": 0.5362462760675273,
      "grad_norm": 0.32427316904067993,
      "learning_rate": 0.00018262150220913108,
      "loss": 1.557,
      "step": 270
    },
    {
      "epoch": 0.5561072492552135,
      "grad_norm": 0.27096110582351685,
      "learning_rate": 0.00018114874815905745,
      "loss": 1.4621,
      "step": 280
    },
    {
      "epoch": 0.5759682224428997,
      "grad_norm": 0.2622559070587158,
      "learning_rate": 0.00017967599410898382,
      "loss": 1.405,
      "step": 290
    },
    {
      "epoch": 0.5958291956305859,
      "grad_norm": 0.218441903591156,
      "learning_rate": 0.00017820324005891018,
      "loss": 1.3539,
      "step": 300
    },
    {
      "epoch": 0.6156901688182721,
      "grad_norm": 0.23379528522491455,
      "learning_rate": 0.00017673048600883653,
      "loss": 1.3451,
      "step": 310
    },
    {
      "epoch": 0.6355511420059583,
      "grad_norm": 0.18720181286334991,
      "learning_rate": 0.0001752577319587629,
      "loss": 1.2468,
      "step": 320
    },
    {
      "epoch": 0.6554121151936445,
      "grad_norm": 0.1950547993183136,
      "learning_rate": 0.00017378497790868926,
      "loss": 1.2051,
      "step": 330
    },
    {
      "epoch": 0.6752730883813307,
      "grad_norm": 0.19008129835128784,
      "learning_rate": 0.00017231222385861563,
      "loss": 1.1679,
      "step": 340
    },
    {
      "epoch": 0.6951340615690169,
      "grad_norm": 0.19569890201091766,
      "learning_rate": 0.00017083946980854197,
      "loss": 1.1099,
      "step": 350
    },
    {
      "epoch": 0.7149950347567031,
      "grad_norm": 0.2011900544166565,
      "learning_rate": 0.00016936671575846834,
      "loss": 1.1183,
      "step": 360
    },
    {
      "epoch": 0.7348560079443893,
      "grad_norm": 0.19733044505119324,
      "learning_rate": 0.0001678939617083947,
      "loss": 1.096,
      "step": 370
    },
    {
      "epoch": 0.7547169811320755,
      "grad_norm": 0.1552058756351471,
      "learning_rate": 0.00016642120765832108,
      "loss": 1.0891,
      "step": 380
    },
    {
      "epoch": 0.7745779543197616,
      "grad_norm": 0.15642790496349335,
      "learning_rate": 0.00016494845360824742,
      "loss": 1.1035,
      "step": 390
    },
    {
      "epoch": 0.7944389275074478,
      "grad_norm": 0.1476908028125763,
      "learning_rate": 0.0001634756995581738,
      "loss": 1.0338,
      "step": 400
    },
    {
      "epoch": 0.814299900695134,
      "grad_norm": 0.13390472531318665,
      "learning_rate": 0.00016200294550810016,
      "loss": 0.9956,
      "step": 410
    },
    {
      "epoch": 0.8341608738828202,
      "grad_norm": 0.11002742499113083,
      "learning_rate": 0.00016053019145802653,
      "loss": 1.0091,
      "step": 420
    },
    {
      "epoch": 0.8540218470705064,
      "grad_norm": 0.12080307304859161,
      "learning_rate": 0.00015905743740795287,
      "loss": 0.9958,
      "step": 430
    },
    {
      "epoch": 0.8738828202581926,
      "grad_norm": 0.11783244460821152,
      "learning_rate": 0.00015758468335787924,
      "loss": 0.9934,
      "step": 440
    },
    {
      "epoch": 0.8937437934458788,
      "grad_norm": 0.10725913941860199,
      "learning_rate": 0.0001561119293078056,
      "loss": 0.9864,
      "step": 450
    },
    {
      "epoch": 0.913604766633565,
      "grad_norm": 0.12467178702354431,
      "learning_rate": 0.00015463917525773197,
      "loss": 0.9603,
      "step": 460
    },
    {
      "epoch": 0.9334657398212513,
      "grad_norm": 0.11982572823762894,
      "learning_rate": 0.00015316642120765834,
      "loss": 0.9494,
      "step": 470
    },
    {
      "epoch": 0.9533267130089375,
      "grad_norm": 0.1040925681591034,
      "learning_rate": 0.00015169366715758468,
      "loss": 0.9728,
      "step": 480
    },
    {
      "epoch": 0.9731876861966237,
      "grad_norm": 0.12806542217731476,
      "learning_rate": 0.00015022091310751105,
      "loss": 0.9556,
      "step": 490
    },
    {
      "epoch": 0.9930486593843099,
      "grad_norm": 0.08689762651920319,
      "learning_rate": 0.00014874815905743742,
      "loss": 0.9546,
      "step": 500
    },
    {
      "epoch": 1.0119165839126116,
      "grad_norm": 0.1739690601825714,
      "learning_rate": 0.0001472754050073638,
      "loss": 0.867,
      "step": 510
    },
    {
      "epoch": 1.0317775571002978,
      "grad_norm": 0.11353025585412979,
      "learning_rate": 0.00014580265095729013,
      "loss": 0.9483,
      "step": 520
    },
    {
      "epoch": 1.051638530287984,
      "grad_norm": 0.08877526968717575,
      "learning_rate": 0.0001443298969072165,
      "loss": 0.9301,
      "step": 530
    },
    {
      "epoch": 1.0714995034756702,
      "grad_norm": 0.08897541463375092,
      "learning_rate": 0.00014285714285714287,
      "loss": 0.9481,
      "step": 540
    },
    {
      "epoch": 1.0913604766633564,
      "grad_norm": 0.09619913250207901,
      "learning_rate": 0.00014138438880706924,
      "loss": 0.9602,
      "step": 550
    },
    {
      "epoch": 1.1112214498510427,
      "grad_norm": 0.10377218574285507,
      "learning_rate": 0.00013991163475699558,
      "loss": 0.8838,
      "step": 560
    },
    {
      "epoch": 1.1310824230387289,
      "grad_norm": 0.0885198637843132,
      "learning_rate": 0.00013843888070692195,
      "loss": 0.9272,
      "step": 570
    },
    {
      "epoch": 1.150943396226415,
      "grad_norm": 0.09900423139333725,
      "learning_rate": 0.00013696612665684832,
      "loss": 0.9172,
      "step": 580
    },
    {
      "epoch": 1.1708043694141013,
      "grad_norm": 0.10100755840539932,
      "learning_rate": 0.00013549337260677468,
      "loss": 0.9318,
      "step": 590
    },
    {
      "epoch": 1.1906653426017875,
      "grad_norm": 0.09152551740407944,
      "learning_rate": 0.00013402061855670103,
      "loss": 0.8972,
      "step": 600
    },
    {
      "epoch": 1.2105263157894737,
      "grad_norm": 0.09282821416854858,
      "learning_rate": 0.0001325478645066274,
      "loss": 0.9343,
      "step": 610
    },
    {
      "epoch": 1.2303872889771599,
      "grad_norm": 0.08163782954216003,
      "learning_rate": 0.00013107511045655376,
      "loss": 0.8728,
      "step": 620
    },
    {
      "epoch": 1.250248262164846,
      "grad_norm": 0.0785212591290474,
      "learning_rate": 0.00012960235640648013,
      "loss": 0.9609,
      "step": 630
    },
    {
      "epoch": 1.2701092353525323,
      "grad_norm": 0.10189692676067352,
      "learning_rate": 0.0001281296023564065,
      "loss": 0.8933,
      "step": 640
    },
    {
      "epoch": 1.2899702085402185,
      "grad_norm": 0.09451957792043686,
      "learning_rate": 0.00012665684830633284,
      "loss": 0.8891,
      "step": 650
    },
    {
      "epoch": 1.3098311817279047,
      "grad_norm": 0.7394144535064697,
      "learning_rate": 0.0001251840942562592,
      "loss": 0.9038,
      "step": 660
    },
    {
      "epoch": 1.329692154915591,
      "grad_norm": 0.08540357649326324,
      "learning_rate": 0.00012371134020618558,
      "loss": 0.8862,
      "step": 670
    },
    {
      "epoch": 1.349553128103277,
      "grad_norm": 0.11498885601758957,
      "learning_rate": 0.00012223858615611195,
      "loss": 0.893,
      "step": 680
    },
    {
      "epoch": 1.3694141012909633,
      "grad_norm": 0.07667280733585358,
      "learning_rate": 0.00012076583210603829,
      "loss": 0.8838,
      "step": 690
    },
    {
      "epoch": 1.3892750744786495,
      "grad_norm": 0.074873186647892,
      "learning_rate": 0.00011929307805596466,
      "loss": 0.8754,
      "step": 700
    },
    {
      "epoch": 1.4091360476663357,
      "grad_norm": 0.10548407584428787,
      "learning_rate": 0.00011782032400589103,
      "loss": 0.8695,
      "step": 710
    },
    {
      "epoch": 1.428997020854022,
      "grad_norm": 0.0891699567437172,
      "learning_rate": 0.0001163475699558174,
      "loss": 0.8947,
      "step": 720
    },
    {
      "epoch": 1.4488579940417081,
      "grad_norm": 0.0781635269522667,
      "learning_rate": 0.00011487481590574374,
      "loss": 0.8886,
      "step": 730
    },
    {
      "epoch": 1.4687189672293943,
      "grad_norm": 0.09771183133125305,
      "learning_rate": 0.0001134020618556701,
      "loss": 0.8695,
      "step": 740
    },
    {
      "epoch": 1.4885799404170803,
      "grad_norm": 0.12222646921873093,
      "learning_rate": 0.00011192930780559647,
      "loss": 0.8958,
      "step": 750
    },
    {
      "epoch": 1.5084409136047667,
      "grad_norm": 0.14306539297103882,
      "learning_rate": 0.00011045655375552284,
      "loss": 0.9065,
      "step": 760
    },
    {
      "epoch": 1.5283018867924527,
      "grad_norm": 0.0808381736278534,
      "learning_rate": 0.00010898379970544918,
      "loss": 0.9061,
      "step": 770
    },
    {
      "epoch": 1.5481628599801391,
      "grad_norm": 0.08324909955263138,
      "learning_rate": 0.00010751104565537555,
      "loss": 0.881,
      "step": 780
    },
    {
      "epoch": 1.5680238331678251,
      "grad_norm": 0.08286873996257782,
      "learning_rate": 0.00010603829160530192,
      "loss": 0.8792,
      "step": 790
    },
    {
      "epoch": 1.5878848063555115,
      "grad_norm": 0.07929530739784241,
      "learning_rate": 0.00010456553755522829,
      "loss": 0.8513,
      "step": 800
    },
    {
      "epoch": 1.6077457795431975,
      "grad_norm": 0.08240831643342972,
      "learning_rate": 0.00010309278350515463,
      "loss": 0.8601,
      "step": 810
    },
    {
      "epoch": 1.627606752730884,
      "grad_norm": 0.06192825362086296,
      "learning_rate": 0.000101620029455081,
      "loss": 0.8345,
      "step": 820
    },
    {
      "epoch": 1.64746772591857,
      "grad_norm": 0.06315047293901443,
      "learning_rate": 0.00010014727540500737,
      "loss": 0.8424,
      "step": 830
    },
    {
      "epoch": 1.6673286991062564,
      "grad_norm": 0.0636216402053833,
      "learning_rate": 9.867452135493374e-05,
      "loss": 0.8635,
      "step": 840
    },
    {
      "epoch": 1.6871896722939423,
      "grad_norm": 0.06389123946428299,
      "learning_rate": 9.720176730486009e-05,
      "loss": 0.861,
      "step": 850
    },
    {
      "epoch": 1.7070506454816285,
      "grad_norm": 0.060354217886924744,
      "learning_rate": 9.572901325478646e-05,
      "loss": 0.8614,
      "step": 860
    },
    {
      "epoch": 1.7269116186693148,
      "grad_norm": 0.06315994262695312,
      "learning_rate": 9.425625920471282e-05,
      "loss": 0.8924,
      "step": 870
    },
    {
      "epoch": 1.746772591857001,
      "grad_norm": 0.06572328507900238,
      "learning_rate": 9.278350515463918e-05,
      "loss": 0.8702,
      "step": 880
    },
    {
      "epoch": 1.7666335650446872,
      "grad_norm": 0.06933125853538513,
      "learning_rate": 9.131075110456554e-05,
      "loss": 0.885,
      "step": 890
    },
    {
      "epoch": 1.7864945382323734,
      "grad_norm": 0.10287737101316452,
      "learning_rate": 8.983799705449191e-05,
      "loss": 0.8787,
      "step": 900
    },
    {
      "epoch": 1.8063555114200596,
      "grad_norm": 0.07607313990592957,
      "learning_rate": 8.836524300441826e-05,
      "loss": 0.8336,
      "step": 910
    },
    {
      "epoch": 1.8262164846077458,
      "grad_norm": 0.061281200498342514,
      "learning_rate": 8.689248895434463e-05,
      "loss": 0.8323,
      "step": 920
    },
    {
      "epoch": 1.846077457795432,
      "grad_norm": 0.1097320020198822,
      "learning_rate": 8.541973490427099e-05,
      "loss": 0.8482,
      "step": 930
    },
    {
      "epoch": 1.8659384309831182,
      "grad_norm": 0.08060848712921143,
      "learning_rate": 8.394698085419735e-05,
      "loss": 0.843,
      "step": 940
    },
    {
      "epoch": 1.8857994041708044,
      "grad_norm": 0.0615239217877388,
      "learning_rate": 8.247422680412371e-05,
      "loss": 0.8829,
      "step": 950
    },
    {
      "epoch": 1.9056603773584906,
      "grad_norm": 0.07425931096076965,
      "learning_rate": 8.100147275405008e-05,
      "loss": 0.8358,
      "step": 960
    },
    {
      "epoch": 1.9255213505461768,
      "grad_norm": 0.08096233755350113,
      "learning_rate": 7.952871870397643e-05,
      "loss": 0.8202,
      "step": 970
    },
    {
      "epoch": 1.945382323733863,
      "grad_norm": 0.06500227749347687,
      "learning_rate": 7.80559646539028e-05,
      "loss": 0.8567,
      "step": 980
    },
    {
      "epoch": 1.9652432969215492,
      "grad_norm": 0.06877470761537552,
      "learning_rate": 7.658321060382917e-05,
      "loss": 0.8728,
      "step": 990
    },
    {
      "epoch": 1.9851042701092354,
      "grad_norm": 0.06452488899230957,
      "learning_rate": 7.511045655375553e-05,
      "loss": 0.8231,
      "step": 1000
    },
    {
      "epoch": 2.0039721946375373,
      "grad_norm": 0.07451576739549637,
      "learning_rate": 7.36377025036819e-05,
      "loss": 0.811,
      "step": 1010
    },
    {
      "epoch": 2.0238331678252233,
      "grad_norm": 0.0724850445985794,
      "learning_rate": 7.216494845360825e-05,
      "loss": 0.8705,
      "step": 1020
    },
    {
      "epoch": 2.0436941410129097,
      "grad_norm": 0.06382621824741364,
      "learning_rate": 7.069219440353462e-05,
      "loss": 0.8852,
      "step": 1030
    },
    {
      "epoch": 2.0635551142005957,
      "grad_norm": 0.05842818692326546,
      "learning_rate": 6.921944035346097e-05,
      "loss": 0.852,
      "step": 1040
    },
    {
      "epoch": 2.083416087388282,
      "grad_norm": 0.09603419899940491,
      "learning_rate": 6.774668630338734e-05,
      "loss": 0.8192,
      "step": 1050
    },
    {
      "epoch": 2.103277060575968,
      "grad_norm": 0.059414420276880264,
      "learning_rate": 6.62739322533137e-05,
      "loss": 0.8416,
      "step": 1060
    },
    {
      "epoch": 2.1231380337636545,
      "grad_norm": 0.05702930688858032,
      "learning_rate": 6.480117820324007e-05,
      "loss": 0.8382,
      "step": 1070
    },
    {
      "epoch": 2.1429990069513405,
      "grad_norm": 0.06360898911952972,
      "learning_rate": 6.332842415316642e-05,
      "loss": 0.8312,
      "step": 1080
    },
    {
      "epoch": 2.162859980139027,
      "grad_norm": 0.06687271595001221,
      "learning_rate": 6.185567010309279e-05,
      "loss": 0.8451,
      "step": 1090
    },
    {
      "epoch": 2.182720953326713,
      "grad_norm": 0.06802119314670563,
      "learning_rate": 6.0382916053019144e-05,
      "loss": 0.8367,
      "step": 1100
    },
    {
      "epoch": 2.2025819265143993,
      "grad_norm": 0.06538064032793045,
      "learning_rate": 5.891016200294551e-05,
      "loss": 0.8292,
      "step": 1110
    },
    {
      "epoch": 2.2224428997020853,
      "grad_norm": 0.06991787254810333,
      "learning_rate": 5.743740795287187e-05,
      "loss": 0.8632,
      "step": 1120
    },
    {
      "epoch": 2.2423038728897717,
      "grad_norm": 0.07166291028261185,
      "learning_rate": 5.596465390279824e-05,
      "loss": 0.8348,
      "step": 1130
    },
    {
      "epoch": 2.2621648460774577,
      "grad_norm": 0.06218932569026947,
      "learning_rate": 5.449189985272459e-05,
      "loss": 0.8278,
      "step": 1140
    },
    {
      "epoch": 2.282025819265144,
      "grad_norm": 0.06290316581726074,
      "learning_rate": 5.301914580265096e-05,
      "loss": 0.846,
      "step": 1150
    },
    {
      "epoch": 2.30188679245283,
      "grad_norm": 0.0652083307504654,
      "learning_rate": 5.1546391752577315e-05,
      "loss": 0.8761,
      "step": 1160
    },
    {
      "epoch": 2.3217477656405165,
      "grad_norm": 0.07116752862930298,
      "learning_rate": 5.0073637702503684e-05,
      "loss": 0.8012,
      "step": 1170
    },
    {
      "epoch": 2.3416087388282025,
      "grad_norm": 0.06463521718978882,
      "learning_rate": 4.8600883652430046e-05,
      "loss": 0.8153,
      "step": 1180
    },
    {
      "epoch": 2.361469712015889,
      "grad_norm": 0.18544405698776245,
      "learning_rate": 4.712812960235641e-05,
      "loss": 0.8234,
      "step": 1190
    },
    {
      "epoch": 2.381330685203575,
      "grad_norm": 0.07442730665206909,
      "learning_rate": 4.565537555228277e-05,
      "loss": 0.8174,
      "step": 1200
    },
    {
      "epoch": 2.4011916583912614,
      "grad_norm": 0.07359892129898071,
      "learning_rate": 4.418262150220913e-05,
      "loss": 0.8217,
      "step": 1210
    },
    {
      "epoch": 2.4210526315789473,
      "grad_norm": 0.06064242497086525,
      "learning_rate": 4.270986745213549e-05,
      "loss": 0.8219,
      "step": 1220
    },
    {
      "epoch": 2.4409136047666333,
      "grad_norm": 0.06731200963258743,
      "learning_rate": 4.1237113402061855e-05,
      "loss": 0.8769,
      "step": 1230
    },
    {
      "epoch": 2.4607745779543198,
      "grad_norm": 0.07442102581262589,
      "learning_rate": 3.976435935198822e-05,
      "loss": 0.8204,
      "step": 1240
    },
    {
      "epoch": 2.480635551142006,
      "grad_norm": 0.05897661671042442,
      "learning_rate": 3.8291605301914585e-05,
      "loss": 0.8568,
      "step": 1250
    },
    {
      "epoch": 2.500496524329692,
      "grad_norm": 0.05957851558923721,
      "learning_rate": 3.681885125184095e-05,
      "loss": 0.8446,
      "step": 1260
    },
    {
      "epoch": 2.520357497517378,
      "grad_norm": 0.06821189820766449,
      "learning_rate": 3.534609720176731e-05,
      "loss": 0.847,
      "step": 1270
    },
    {
      "epoch": 2.5402184707050646,
      "grad_norm": 0.07137560844421387,
      "learning_rate": 3.387334315169367e-05,
      "loss": 0.8069,
      "step": 1280
    },
    {
      "epoch": 2.560079443892751,
      "grad_norm": 0.062374282628297806,
      "learning_rate": 3.240058910162003e-05,
      "loss": 0.8216,
      "step": 1290
    },
    {
      "epoch": 2.579940417080437,
      "grad_norm": 0.06410329788923264,
      "learning_rate": 3.0927835051546395e-05,
      "loss": 0.8542,
      "step": 1300
    },
    {
      "epoch": 2.599801390268123,
      "grad_norm": 0.06381893903017044,
      "learning_rate": 2.9455081001472756e-05,
      "loss": 0.8547,
      "step": 1310
    },
    {
      "epoch": 2.6196623634558094,
      "grad_norm": 0.060332510620355606,
      "learning_rate": 2.798232695139912e-05,
      "loss": 0.8338,
      "step": 1320
    },
    {
      "epoch": 2.6395233366434954,
      "grad_norm": 0.06320607662200928,
      "learning_rate": 2.650957290132548e-05,
      "loss": 0.8303,
      "step": 1330
    },
    {
      "epoch": 2.659384309831182,
      "grad_norm": 0.06466972827911377,
      "learning_rate": 2.5036818851251842e-05,
      "loss": 0.838,
      "step": 1340
    },
    {
      "epoch": 2.6792452830188678,
      "grad_norm": 0.06776636838912964,
      "learning_rate": 2.3564064801178204e-05,
      "loss": 0.8356,
      "step": 1350
    },
    {
      "epoch": 2.699106256206554,
      "grad_norm": 0.06242155656218529,
      "learning_rate": 2.2091310751104566e-05,
      "loss": 0.822,
      "step": 1360
    },
    {
      "epoch": 2.71896722939424,
      "grad_norm": 0.07879174500703812,
      "learning_rate": 2.0618556701030927e-05,
      "loss": 0.846,
      "step": 1370
    },
    {
      "epoch": 2.7388282025819266,
      "grad_norm": 0.0744870975613594,
      "learning_rate": 1.9145802650957293e-05,
      "loss": 0.8593,
      "step": 1380
    },
    {
      "epoch": 2.7586891757696126,
      "grad_norm": 0.05912059172987938,
      "learning_rate": 1.7673048600883655e-05,
      "loss": 0.8295,
      "step": 1390
    },
    {
      "epoch": 2.778550148957299,
      "grad_norm": 0.053457070142030716,
      "learning_rate": 1.6200294550810016e-05,
      "loss": 0.8431,
      "step": 1400
    },
    {
      "epoch": 2.798411122144985,
      "grad_norm": 0.06523022800683975,
      "learning_rate": 1.4727540500736378e-05,
      "loss": 0.8061,
      "step": 1410
    },
    {
      "epoch": 2.8182720953326714,
      "grad_norm": 0.05829358845949173,
      "learning_rate": 1.325478645066274e-05,
      "loss": 0.8342,
      "step": 1420
    },
    {
      "epoch": 2.8381330685203574,
      "grad_norm": 0.05817714333534241,
      "learning_rate": 1.1782032400589102e-05,
      "loss": 0.8288,
      "step": 1430
    },
    {
      "epoch": 2.857994041708044,
      "grad_norm": 0.0629628375172615,
      "learning_rate": 1.0309278350515464e-05,
      "loss": 0.8461,
      "step": 1440
    },
    {
      "epoch": 2.87785501489573,
      "grad_norm": 0.058125920593738556,
      "learning_rate": 8.836524300441827e-06,
      "loss": 0.798,
      "step": 1450
    },
    {
      "epoch": 2.8977159880834162,
      "grad_norm": 0.06997444480657578,
      "learning_rate": 7.363770250368189e-06,
      "loss": 0.8013,
      "step": 1460
    },
    {
      "epoch": 2.917576961271102,
      "grad_norm": 0.06737794727087021,
      "learning_rate": 5.891016200294551e-06,
      "loss": 0.8349,
      "step": 1470
    },
    {
      "epoch": 2.9374379344587886,
      "grad_norm": 0.05669743940234184,
      "learning_rate": 4.418262150220914e-06,
      "loss": 0.8309,
      "step": 1480
    },
    {
      "epoch": 2.9572989076464746,
      "grad_norm": 0.06273474544286728,
      "learning_rate": 2.9455081001472755e-06,
      "loss": 0.8441,
      "step": 1490
    },
    {
      "epoch": 2.9771598808341606,
      "grad_norm": 0.058247435837984085,
      "learning_rate": 1.4727540500736377e-06,
      "loss": 0.8104,
      "step": 1500
    }
  ],
  "logging_steps": 10,
  "max_steps": 1509,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.297406343970816e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
